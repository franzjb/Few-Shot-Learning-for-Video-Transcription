# Few-Shot-Learning-for-Video-Transcription

## German Lip-Reading with PyTorch AVSR and GLips Dataset

## ğŸ“Œ Overview  
This project adapts the **PyTorch-based Audio-Visual Speech Recognition (AVSR)** system for **German lip-reading**, utilizing the **GLips dataset**. The adaptation involves **modifying preprocessing, training pipelines, and model architecture** to better capture **German phonetic structures and articulation patterns**.

## ğŸš€ Key Features

ğŸ”¹ **GLips Dataset Integration** â€“ Processed parliamentary speech videos for robust lip-reading.  

ğŸ”¹ **SentencePiece Tokenization** â€“ Adapted the SPM model to German phoneme distributions.  

ğŸ”¹ **Multimodal Learning** â€“ Utilized both visual (lip movements) and audio cues.  

ğŸ”¹ **Optimized Training Pipeline** â€“ Custom data augmentation, batch handling, and DDP-based multi-GPU training.  

ğŸ”¹ **Real-Time Inference** â€“ Implemented efficient frame-wise video processing.  

## ğŸ›  Installation  

### ğŸ’» Clone the Repository  
```bash
git clone https://github.com/franzjb/Few-Shot-Learning-for-Video-Transcription.git
cd lipreading_fsl
```
### ğŸ“¦ Install PyTorch and all necessary packages
```bash
pip install torch torchvision torchaudio pytorch-lightning sentencepiece
```
### ğŸ— Preprocess Dataset
[Pre-Processing](https://github.com/pytorch/audio/tree/main/examples/avsr/data_prep)

### ğŸ“‰ Start Trainig and Evaluation
[Training](https://github.com/pytorch/audio/tree/main/examples/avsr)


### ğŸ“ As reference

Real-time ASR/VSR/AV-ASR Examples (PyTorch) [https://github.com/username/another-repo](https://github.com/pytorch/audio/tree/main/examples/avsr)

GLips - German Lipreading Dataset (University of Hamburg) [https://www.fdr.uni-hamburg.de/record/10048](https://www.fdr.uni-hamburg.de/record/10048)


## ğŸ¯ Evaluation

## ğŸ“¬ Contact

For inquiries or contributions, feel free to reach out or open a pull request.

- ğŸ”— **GitHub:** [franzjb](https://github.com/franzjb)
- ğŸ“§ **Email:** franzjb@icloud.com
